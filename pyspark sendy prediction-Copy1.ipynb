{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "\n",
    "#Create a folder for the experiment files\n",
    "training_folder = 'sendy-script'\n",
    "os.makedirs(training_folder, exist_ok=True)\n",
    "\n",
    "#Copy the data file into the experiment folder\n",
    "csv_names = [\"Train(1).csv\", \"Riders.csv\"]\n",
    "\n",
    "for csv in csv_names:\n",
    "    shutil.copy(csv, os.path.join(training_folder,csv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'findspark'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-91f399e15be8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mfindspark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mfindspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/nelsonchris/spark-2.4.5-bin-hadoop2.7'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'findspark'"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init('/home/nelsonchris/spark-2.4.5-bin-hadoop2.7')\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark\n",
    "from pyspark.sql.functions import hour, dayofweek\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType, StringType, DoubleType\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from math import radians, cos, sin, atan, sqrt, atan2, degrees\n",
    "print(\"Libraries imported\")\n",
    "\n",
    "spark = SparkSession.builder.appName('Sendy_logistics').getOrCreate()\n",
    "print(\"Spark session created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to sendy-script/sendy.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a $training_folder/sendy.py\n",
    "print(\"====loading data=====\")\n",
    "data = spark.read.csv('Train(1).csv', inferSchema=True,header=True)\n",
    "rider = spark.read.csv('Riders.csv', inferSchema=True,header=True)\n",
    "print(\"=====Data loaded=====\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28269"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.count() + test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7068"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------+------------+-------------+--------------------+\n",
      "|      Order No|     User Id|Vehicle Type|Platform Type|Personal or Business|\n",
      "+--------------+------------+------------+-------------+--------------------+\n",
      "| Order_No_4211| User_Id_633|        Bike|            3|            Business|\n",
      "|Order_No_25375|User_Id_2285|        Bike|            3|            Personal|\n",
      "| Order_No_1899| User_Id_265|        Bike|            3|            Business|\n",
      "+--------------+------------+------------+-------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select(['Order No','User Id','Vehicle Type','Platform Type','Personal or Business']).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Order No: string (nullable = true)\n",
      " |-- User Id: string (nullable = true)\n",
      " |-- Vehicle Type: string (nullable = true)\n",
      " |-- Platform Type: integer (nullable = true)\n",
      " |-- Personal or Business: string (nullable = true)\n",
      " |-- Placement - Day of Month: integer (nullable = true)\n",
      " |-- Placement - Weekday (Mo = 1): integer (nullable = true)\n",
      " |-- Placement - Time: timestamp (nullable = true)\n",
      " |-- Confirmation - Day of Month: integer (nullable = true)\n",
      " |-- Confirmation - Weekday (Mo = 1): integer (nullable = true)\n",
      " |-- Confirmation - Time: timestamp (nullable = true)\n",
      " |-- Arrival at Pickup - Day of Month: integer (nullable = true)\n",
      " |-- Arrival at Pickup - Weekday (Mo = 1): integer (nullable = true)\n",
      " |-- Arrival at Pickup - Time: timestamp (nullable = true)\n",
      " |-- Pickup - Day of Month: integer (nullable = true)\n",
      " |-- Pickup - Weekday (Mo = 1): integer (nullable = true)\n",
      " |-- Pickup - Time: timestamp (nullable = true)\n",
      " |-- Arrival at Destination - Day of Month: integer (nullable = true)\n",
      " |-- Arrival at Destination - Weekday (Mo = 1): integer (nullable = true)\n",
      " |-- Arrival at Destination - Time: string (nullable = true)\n",
      " |-- Distance (KM): integer (nullable = true)\n",
      " |-- Temperature: double (nullable = true)\n",
      " |-- Precipitation in millimeters: double (nullable = true)\n",
      " |-- Pickup Lat: double (nullable = true)\n",
      " |-- Pickup Long: double (nullable = true)\n",
      " |-- Destination Lat: double (nullable = true)\n",
      " |-- Destination Long: double (nullable = true)\n",
      " |-- Rider Id: string (nullable = true)\n",
      " |-- Time from Pickup to Arrival: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to sendy-script/sendy.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a $training_folder/sendy.py\n",
    "#Merge rider data to both train and test\n",
    "data_merge = data.join(rider, on=['Rider Id'],how ='inner')\n",
    "print(\"Megered data and rider together\")\n",
    "\n",
    "cols_to_drop = ['Vehicle Type','Order No','Arrival at Destination - Day of Month',\n",
    "       'Arrival at Destination - Weekday (Mo = 1)',\n",
    "        'Arrival at Destination - Time','Precipitation in millimeters',\n",
    "                'Temperature','Rider Id', \"User Id\", ]\n",
    "\n",
    "time_cols = ['Placement - Time',\n",
    "             'Confirmation - Time',\n",
    "             'Arrival at Pickup - Time',\n",
    "             'Pickup - Time']\n",
    "\n",
    "data_merge = data_merge.drop(*cols_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spark_shape(self):\n",
    "    return (self.count(), len(self.columns))\n",
    "pyspark.sql.dataframe.DataFrame.shape = spark_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21201, 24)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_merge.shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a $training_folder/sendy.py\n",
    "\n",
    "#Extraxt hour and day of the week data from date columns\n",
    "\n",
    "for col in time_cols:\n",
    "    train = train.withColumn(col+\"_Hour\", hour(col))\n",
    "print(\"Tine columns extracted\")\n",
    "\n",
    "#Calculate the harvsine distance\n",
    "def harvsine_array(lat1, lng1 , lat2, lng2):\n",
    "    lat1, lng1, lat2, lng2 = map(radians, (lat1, lng1 , lat2, lng2))\n",
    "    AVG_EARTH_RADIUS = 6371 #in kilometers\n",
    "    lat = lat2 - lat1\n",
    "    lng = lng2 - lng1\n",
    "    d = sin(lat * 0.5) ** 2 + cos(lat1) * cos(lat2) * sin(lng * 0.5) ** 2\n",
    "    h = 2 * AVG_EARTH_RADIUS * atan(sqrt(d))\n",
    "    return h\n",
    "\n",
    "\n",
    "# Manhattan distance is the sum of the horizontal and vertical distance between points on a grid\n",
    "def manhattan_dist(lat1 ,lng1, lat2, lng2):\n",
    "    a = harvsine_array(lat1, lng1 , lat1 , lng2)\n",
    "    b = harvsine_array(lat1,lng1, lat2, lng1)\n",
    "    \n",
    "    return a + b\n",
    "\n",
    "#Direction from the given coordinates\n",
    "def bearing_array(lat1, lng1, lat2, lng2):\n",
    "    AVG_EARTH_RADIUS = 6371  # in km\n",
    "    lng_delta_rad = radians(lng2 - lng1)\n",
    "    lat1, lng1, lat2, lng2 = map(radians, (lat1, lng1, lat2, lng2))\n",
    "    y = sin(lng_delta_rad) * cos(lat2)\n",
    "    x = cos(lat1) * sin(lat2) - sin(lat1) * cos(lat2) * cos(lng_delta_rad)\n",
    "    return degrees(atan2(y, x))\n",
    "\n",
    "#Register the created functions\n",
    "HarvsineUDF = udf(lambda a,b,c,d: harvsine_array(a,b,c,d),StringType())\n",
    "ManhattanUDF = udf(lambda a,b,c,d: manhattan_dist(a,b,c,d),StringType())\n",
    "BearingUDF = udf(lambda a,b,c,d: bearing_array(a,b,c,d),StringType())\n",
    "\n",
    "print(\"Applying feature enginerring to dataframe\")\n",
    "# Call the harvsine function\n",
    "data_merge = data_merge.withColumn(\"Harvsine Distance\", HarvsineUDF(\"Pickup Lat\", \"Pickup Long\", \"Destination Lat\", \"Destination Long\").cast(DoubleType()))\n",
    "\n",
    "# Call the manhattan dist function\n",
    "data_merge = data_merge.withColumn(\"Manhattan Distance\", ManhattanUDF(\"Pickup Lat\", \"Pickup Long\", \"Destination Lat\", \"Destination Long\").cast(DoubleType()))\n",
    "\n",
    "# Call the bearing fnuction\n",
    "data_merge = data_merge.withColumn(\"Direction Distance\", BearingUDF(\"Pickup Lat\", \"Pickup Long\", \"Destination Lat\", \"Destination Long\").cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9303875732130382"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "harvsine_array(-1.317755, 36.830370, -1.300406, 36.829741)#,manhattan_dist(-1.351453, 36.830370, -1.300406, 36.829741),bearing_array(-1.351453, 36.830370, -1.300406, 36.829741)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Applying feature enginerring to dataframe\")\n",
    "# Call the harvsine function\n",
    "data_merge = data_merge.withColumn(\"Harvsine Distance\", HarvsineUDF(\"Pickup Lat\", \"Pickup Long\", \"Destination Lat\", \"Destination Long\").cast(DoubleType()))\n",
    "\n",
    "# Call the manhattan dist function\n",
    "data_merge = data_merge.withColumn(\"Manhattan Distance\", ManhattanUDF(\"Pickup Lat\", \"Pickup Long\", \"Destination Lat\", \"Destination Long\").cast(DoubleType()))\n",
    "\n",
    "# Call the bearing fnuction\n",
    "data_merge = data_merge.withColumn(\"Direction Distance\", BearingUDF(\"Pickup Lat\", \"Pickup Long\", \"Destination Lat\", \"Destination Long\").cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+-------------------+\n",
      "| Harvsine Distance|Manhattan Distance| Direction Distance|\n",
      "+------------------+------------------+-------------------+\n",
      "|1.9303332205710244| 1.999021623913786|-2.0769034077507285|\n",
      "|11.339844530356967|15.720954182392934|-56.392163212783096|\n",
      "|1.8800787264224066|2.5111852071397336| -64.18386563483746|\n",
      "| 4.943458069180657| 6.835993082860763| -57.09155293419641|\n",
      "|3.7248284348567235| 5.130330919876971| 148.11439790865393|\n",
      "|6.6385397000476445| 9.361638171756846|  40.68337317738699|\n",
      "| 2.907391577671219|3.2041393518007317|  83.80566320374294|\n",
      "| 2.030743156688254|2.2793926801519393| -97.53156300361854|\n",
      "| 6.960283546786541| 9.841714698013039|  136.0513388990286|\n",
      "| 9.305257529847268|10.323211038893358|  83.32990760598942|\n",
      "|   4.3124483007969| 4.390565812048895|-1.0475300519138007|\n",
      "|10.756208553054682|13.378440135944835| 106.58157223754336|\n",
      "| 5.325641743113615| 7.036870106880482|  114.1175917599587|\n",
      "|15.183141058098334| 17.94165225881389|-168.32488847148153|\n",
      "| 1.193178246545741| 1.632085441970755| -59.71209369701208|\n",
      "|10.087963990085717| 14.26145525556684| -43.48505278613277|\n",
      "|12.403773918290462| 17.42071123249333|-51.727424532969486|\n",
      "| 8.404422893384357|11.487042154240786| 149.88202832462215|\n",
      "| 4.194015781017971|5.9311904763832075|  135.2557892978611|\n",
      "| 3.697551990815286|3.9697510256311785| -94.39064513416282|\n",
      "+------------------+------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_merge.select(\"Harvsine Distance\", \"Manhattan Distance\", \"Direction Distance\").show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21201, 27)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_merge.shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = data_merge.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_merge = data_merge.drop(*time_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#String indexer to convert cat column to double\n",
    "cols_index = [\"Personal or Business\"]\n",
    "\n",
    "indexers = [StringIndexer(inputCol=column, outputCol=column+\"_index\").fit(data_merge) for column in cols_index]\n",
    "pipeline_idx = Pipeline(stages = indexers)\n",
    "final_df = pipeline_idx.fit(data_merge).transform(data_merge)\n",
    "final_df = final_df.drop(*cols_index)\n",
    "\n",
    "print(\"========String indexer applied========\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,test_data = final_df.randomSplit([0.7,0.3])\n",
    "print(\"Data frame split into train and test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_vector = [cols for cols in test_data.columns if cols != 'Time from Pickup to Arrival']\n",
    "label = test_data.select(\"Time from Pickup to Arrival\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(\n",
    "            inputCols = cols_to_vector,\n",
    "            outputCol = \"features\"\n",
    "    )\n",
    "\n",
    "gbt = GBTRegressor(featuresCol=\"features\", \n",
    "                   labelCol=\"Time from Pickup to Arrival\")\n",
    "# final_df = assembler.transform(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramGrid = ParamGridBuilder()\\\n",
    "            .addGrid(gbt.maxDepth, [2, 5])\\\n",
    "            .addGrid(gbt.maxIter, [10, 50])\\\n",
    "            .build()\n",
    "\n",
    "#Next define evaluation metric.\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\",\n",
    "                                labelCol=gbt.getLabelCol(),\n",
    "                                predictionCol=gbt.getPredictionCol())\n",
    "\n",
    "cv = CrossValidator(estimator=gbt, evaluator=evaluator,\n",
    "                   estimatorParamMaps=paramGrid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[assembler, cv])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"===========Fitting data pipeline===========\")\n",
    "pipeline_model = pipeline.fit(train_data)\n",
    "print(\"========Model trained=========\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pipeline_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "769.2976609811352\n"
     ]
    }
   ],
   "source": [
    "rmse = evaluator.evaluate(predictions)\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"Model\", exist_ok=True)\n",
    "pipeline_model.save(\"Model/v1\")\n",
    "print(\"pipeline saved\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}